# Databricks notebook source
# MAGIC %md
# MAGIC # Parte 2:  Sistema RAG - Procesamiento y Chunking
# MAGIC *   **Autor:** Carolina Torres Zapata
# MAGIC *   **Fecha:** 2025-11-24
# MAGIC *   **Contexto:** Una vez adquirido el dato crudo (Capa Bronce), el paso cr√≠tico en un sistema RAG es la **segmentaci√≥n (Chunking)**. Si cortamos el texto arbitrariamente, rompemos las oraciones y el LLM pierde contexto.
# MAGIC *   **Objetivo del Notebook:**
# MAGIC      1.  **Lectura Raw:** Cargar el archivo de texto desde el Volumen de Unity Catalog.
# MAGIC      2.  **Chunking Sem√°ntico:** Implementar una l√≥gica que respete los p√°rrafos y oraciones, agrup√°ndolos en bloques de tama√±o √≥ptimo (ej. ~1000 caracteres) para la ventana de contexto del LLM.
# MAGIC      3.  **Estructuraci√≥n:** Convertir la lista de textos en una Tabla Delta (Capa Silver) con identificadores √∫nicos (`chunk_id`) para trazabilidad futura.

# COMMAND ----------

# MAGIC %md
# MAGIC ## 1. Importar Librer√≠as
# MAGIC

# COMMAND ----------

import pandas as pd
from pyspark.sql import SparkSession
from pyspark.sql.functions import monotonically_increasing_id

# COMMAND ----------

# MAGIC %md
# MAGIC ## 2. Lectura desde Volumen (Capa Bronce/Raw)
# MAGIC Leemos el archivo directamente desde el almacenamiento gestionado de Databricks
# MAGIC
# MAGIC A diferencia de los procesos ETL tradicionales que leen l√≠nea por l√≠nea (como CSVs), aqu√≠ necesitamos el documento entero como una sola unidad para poder analizar sus p√°rrafos.
# MAGIC Usamos `.option("wholetext", True)` para cargar el contenido completo en una sola fila, preservando los saltos de l√≠nea (`\n`) que son vitales para identificar la estructura del documento.

# COMMAND ----------

ruta_volumen = "/Volumes/dev/bronce/azure_databricks_docs/azure_databricks_intro.txt" 

# 'wholetext' lee todo el archivo en una sola fila de un DataFrame de Spark
df_spark_raw = spark.read.option("wholetext", True).text(ruta_volumen)

print("‚úÖ Texto cargado en Spark DataFrame.")
df_spark_raw.printSchema()

# Extraemos el texto del DataFrame para partirlo
full_text = df_spark_raw.first()[0]

# COMMAND ----------

# MAGIC %md
# MAGIC ## 3. Estrategia de Chunking (Preservaci√≥n de Contexto)
# MAGIC Implementamos una estrategia de **"Ventana Deslizante basada en P√°rrafos"**.
# MAGIC En lugar de cortar ciegamente cada 1000 caracteres (lo que podr√≠a dejar una frase como *"La clave es..."* en un chunk y *"...la seguridad"* en otro), el algoritmo:
# MAGIC
# MAGIC 1.  Divide el texto por p√°rrafos naturales (`\n\n`).
# MAGIC 2.  Agrupa p√°rrafos completos hasta acercarse al l√≠mite de tokens/caracteres (1000).
# MAGIC 3.  **Regla de Calidad:** Filtra l√≠neas de "navegaci√≥n web" o pies de p√°gina cortos (< 50 chars) que son ruido para el modelo.

# COMMAND ----------


# ==========================================
# 1. ESTRATEGIA DE CHUNKING (L√≥gica H√≠brida)
# ==========================================
# Objetivo: Agrupar p√°rrafos completos hasta llegar a un l√≠mite de caracteres.
# Beneficio: Evita cortar frases a la mitad y agrupa t√≠tulos con su contenido.

# Separaci√≥n natural por p√°rrafos
paragraphs = [p.strip() for p in full_text.split("\n\n") if p.strip()]

# Inicializar variables
chunks = []
current_chunk = ""
LIMIT_CHARS = 1000  # L√≠mite m√°ximo por chunk

# Iteraci√≥n para empaquetar p√°rrafos hasta l√≠mite
for para in paragraphs:
    # Filtrar p√°rrafos cortos o de sistema
    if len(para) < 50 or "acceso a esta p√°gina" in para.lower():
        continue
    
    if len(current_chunk) + len(para) + 2 <= LIMIT_CHARS:
        # Agregar p√°rrafo al chunk actual
        if current_chunk:
            current_chunk += "\n\n" + para
        else:
            current_chunk = para
    else:
        # Guardar chunk actual y empezar uno nuevo
        chunks.append(current_chunk)
        current_chunk = para

# Guardar el √∫ltimo chunk
if current_chunk:
    chunks.append(current_chunk)

print(f"üß© Se generaron {len(chunks)} chunks optimizados.")
print(f"   Promedio de caracteres por chunk: {sum(len(c) for c in chunks)/len(chunks):.0f}\n")

# Muestra de primeros 3 chunks
for i, chunk in enumerate(chunks[:3]):
    print(f"Chunk {i+1} ({len(chunk)} chars): {chunk[:200]}...\n")

# COMMAND ----------

# MAGIC %md
# MAGIC ## 4. Estructuraci√≥n y Trazabilidad (Data Modeling)
# MAGIC Para que el sistema RAG funcione, cada fragmento de texto necesita una "c√©dula de identidad".
# MAGIC Generamos un `chunk_id` √∫nico usando `monotonically_increasing_id()`.
# MAGIC *   **Uso posterior:** Cuando el usuario haga una pregunta, el sistema recuperar√° el ID del chunk m√°s relevante.

# COMMAND ----------

# Creamos un DataFrame Spark a partir de la lista de chunks
# Cada chunk ser√° una fila en la columna "chunk_text"
df_chunks = spark.createDataFrame([(c,) for c in chunks], ["chunk_text"])

# Agregamos una columna "chunk_id" con un identificador √∫nico para cada fila
# `monotonically_increasing_id()` genera un ID √∫nico creciente para cada chunk
df_chunks = df_chunks.withColumn("chunk_id", monotonically_increasing_id())

# Confirmamos que el DataFrame se cre√≥ correctamente mostrando las primeras filas
print("‚úÖ DataFrame Spark de chunks creado:")
display(df_chunks)

# COMMAND ----------

# MAGIC %md
# MAGIC ## 5. Guardar tabla Delta en Capa Silver
# MAGIC Guardamos el resultado procesado en `dev.silver.rag_chunks` en formato **Delta**.
# MAGIC Esto permite:
# MAGIC 1.  **Reutilizaci√≥n:** Si falla el proceso de generaci√≥n de embeddings, no hay que volver a leer ni procesar el texto.
# MAGIC 2.  **Schema Enforcement:** Aseguramos que siempre tengamos las columnas `chunk_text` y `chunk_id`.

# COMMAND ----------

target_table = "dev.silver.rag_chunks"

print(f"üíæ Guardando tabla Delta en: {target_table} ...")

df_chunks.write \
    .format("delta") \
    .mode("overwrite") \
    .option("overwriteSchema", "true") \
    .saveAsTable(target_table)

print("‚úÖ Proceso finalizado exitosamente.")