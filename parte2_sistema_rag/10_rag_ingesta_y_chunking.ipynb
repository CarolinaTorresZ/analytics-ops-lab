{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2acda5ad-3ac2-4063-b20c-4e9995a419b2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Parte 2:  Sistema RAG - Procesamiento y Chunking\n",
    "*   **Autor:** Carolina Torres Zapata\n",
    "*   **Fecha:** 2025-11-24\n",
    "*   **Contexto:** Una vez adquirido el dato crudo (Capa Bronce), el paso crítico en un sistema RAG es la **segmentación (Chunking)**. Si cortamos el texto arbitrariamente, rompemos las oraciones y el LLM pierde contexto.\n",
    "*   **Objetivo del Notebook:**\n",
    "     1.  **Lectura Raw:** Cargar el archivo de texto desde el Volumen de Unity Catalog.\n",
    "     2.  **Chunking Semántico:** Implementar una lógica que respete los párrafos y oraciones, agrupándolos en bloques de tamaño óptimo (ej. ~1000 caracteres) para la ventana de contexto del LLM.\n",
    "     3.  **Estructuración:** Convertir la lista de textos en una Tabla Delta (Capa Silver) con identificadores únicos (`chunk_id`) para trazabilidad futura."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "58782529-58a0-417d-a8a6-2986556babb9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 1. Importar Librerías\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bbae5e1c-f561-4a93-905a-52741637e17a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import monotonically_increasing_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "54b148a1-da2c-44d3-b743-5cb4a7c8ee9f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 2. Lectura desde Volumen (Capa Bronce/Raw)\n",
    "Leemos el archivo directamente desde el almacenamiento gestionado de Databricks\n",
    "\n",
    "A diferencia de los procesos ETL tradicionales que leen línea por línea (como CSVs), aquí necesitamos el documento entero como una sola unidad para poder analizar sus párrafos.\n",
    "Usamos `.option(\"wholetext\", True)` para cargar el contenido completo en una sola fila, preservando los saltos de línea (`\\n`) que son vitales para identificar la estructura del documento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cc92fcd7-41fe-44e9-92ac-f0679795e918",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Texto cargado en Spark DataFrame.\nroot\n |-- value: string (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "ruta_volumen = \"/Volumes/dev/bronce/azure_databricks_docs/azure_databricks_intro.txt\" \n",
    "\n",
    "# 'wholetext' lee todo el archivo en una sola fila de un DataFrame de Spark\n",
    "df_spark_raw = spark.read.option(\"wholetext\", True).text(ruta_volumen)\n",
    "\n",
    "print(\"✅ Texto cargado en Spark DataFrame.\")\n",
    "df_spark_raw.printSchema()\n",
    "\n",
    "# Extraemos el texto del DataFrame para partirlo\n",
    "full_text = df_spark_raw.first()[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "42c57b29-34dc-4f3b-9311-692e179e9bce",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 3. Estrategia de Chunking (Preservación de Contexto)\n",
    "Implementamos una estrategia de **\"Ventana Deslizante basada en Párrafos\"**.\n",
    "En lugar de cortar ciegamente cada 1000 caracteres (lo que podría dejar una frase como *\"La clave es...\"* en un chunk y *\"...la seguridad\"* en otro), el algoritmo:\n",
    "\n",
    "1.  Divide el texto por párrafos naturales (`\\n\\n`).\n",
    "2.  Agrupa párrafos completos hasta acercarse al límite de tokens/caracteres (1000).\n",
    "3.  **Regla de Calidad:** Filtra líneas de \"navegación web\" o pies de página cortos (< 50 chars) que son ruido para el modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b1fbba56-396d-477e-8be9-e2e717b33ca6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\uD83E\uDDE9 Se generaron 12 chunks optimizados.\n   Promedio de caracteres por chunk: 741\n\nChunk 1 (927 chars): Azure Databricks es una plataforma de análisis unificada y abierta para crear, implementar, compartir y mantener soluciones de datos, análisis e IA de nivel empresarial a escala. Databricks Data Intel...\n\nChunk 2 (576 chars): Databricks se compromete a la comunidad de código abierto y administra las actualizaciones de integraciones de código abierto con las versiones de Databricks Runtime. Las siguientes tecnologías son pr...\n\nChunk 3 (504 chars): Data Lakehouse combina almacenes de datos empresariales y lagos de datos para acelerar, simplificar y unificar soluciones de datos empresariales. Los ingenieros de datos, los científicos de datos, los...\n\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ==========================================\n",
    "# 1. ESTRATEGIA DE CHUNKING (Lógica Híbrida)\n",
    "# ==========================================\n",
    "# Objetivo: Agrupar párrafos completos hasta llegar a un límite de caracteres.\n",
    "# Beneficio: Evita cortar frases a la mitad y agrupa títulos con su contenido.\n",
    "\n",
    "# Separación natural por párrafos\n",
    "paragraphs = [p.strip() for p in full_text.split(\"\\n\\n\") if p.strip()]\n",
    "\n",
    "# Inicializar variables\n",
    "chunks = []\n",
    "current_chunk = \"\"\n",
    "LIMIT_CHARS = 1000  # Límite máximo por chunk\n",
    "\n",
    "# Iteración para empaquetar párrafos hasta límite\n",
    "for para in paragraphs:\n",
    "    # Filtrar párrafos cortos o de sistema\n",
    "    if len(para) < 50 or \"acceso a esta página\" in para.lower():\n",
    "        continue\n",
    "    \n",
    "    if len(current_chunk) + len(para) + 2 <= LIMIT_CHARS:\n",
    "        # Agregar párrafo al chunk actual\n",
    "        if current_chunk:\n",
    "            current_chunk += \"\\n\\n\" + para\n",
    "        else:\n",
    "            current_chunk = para\n",
    "    else:\n",
    "        # Guardar chunk actual y empezar uno nuevo\n",
    "        chunks.append(current_chunk)\n",
    "        current_chunk = para\n",
    "\n",
    "# Guardar el último chunk\n",
    "if current_chunk:\n",
    "    chunks.append(current_chunk)\n",
    "\n",
    "print(f\"\uD83E\uDDE9 Se generaron {len(chunks)} chunks optimizados.\")\n",
    "print(f\"   Promedio de caracteres por chunk: {sum(len(c) for c in chunks)/len(chunks):.0f}\\n\")\n",
    "\n",
    "# Muestra de primeros 3 chunks\n",
    "for i, chunk in enumerate(chunks[:3]):\n",
    "    print(f\"Chunk {i+1} ({len(chunk)} chars): {chunk[:200]}...\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d51bf128-2113-47a3-a877-d47e0d53e2d3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 4. Estructuración y Trazabilidad (Data Modeling)\n",
    "Para que el sistema RAG funcione, cada fragmento de texto necesita una \"cédula de identidad\".\n",
    "Generamos un `chunk_id` único usando `monotonically_increasing_id()`.\n",
    "*   **Uso posterior:** Cuando el usuario haga una pregunta, el sistema recuperará el ID del chunk más relevante."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e16ddfa7-38ac-4959-aa4c-1951b23196ee",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ DataFrame Spark de chunks creado:\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>chunk_text</th><th>chunk_id</th></tr></thead><tbody><tr><td>Azure Databricks es una plataforma de análisis unificada y abierta para crear, implementar, compartir y mantener soluciones de datos, análisis e IA de nivel empresarial a escala. Databricks Data Intelligence Platform se integra con el almacenamiento en la nube y la seguridad de su cuenta en la nube, y administra e implementa la infraestructura en la nube para usted.\n",
       "\n",
       "Azure Databricks usa inteligencia artificial generativa con el almacén de lago de datos para comprender la semántica única de los datos. A continuación, optimiza automáticamente el rendimiento y administra la infraestructura para adaptarla a las necesidades de su empresa.\n",
       "\n",
       "El procesamiento de lenguaje natural aprende el idioma de su empresa, por lo que puede buscar y detectar datos haciendo una pregunta en sus propias palabras. La asistencia del lenguaje natural le ayuda a escribir código, solucionar errores y encontrar respuestas en la documentación.</td><td>0</td></tr><tr><td>Databricks se compromete a la comunidad de código abierto y administra las actualizaciones de integraciones de código abierto con las versiones de Databricks Runtime. Las siguientes tecnologías son proyectos de código abierto que crearon al principio empleados de Databricks:\n",
       "\n",
       "Los siguientes casos de uso resaltan algunas de las formas en que los clientes usan Azure Databricks para realizar tareas esenciales para procesar, almacenar y analizar los datos que impulsan las decisiones y las funciones empresariales críticas.\n",
       "\n",
       "Creación de un almacén de lago de datos empresarial</td><td>1</td></tr><tr><td>Data Lakehouse combina almacenes de datos empresariales y lagos de datos para acelerar, simplificar y unificar soluciones de datos empresariales. Los ingenieros de datos, los científicos de datos, los analistas y los sistemas de producción pueden usar el data lakehouse como origen único de verdad, lo que proporciona acceso a datos coherentes y reduce las complejidades de creación, mantenimiento y sincronización de muchos sistemas de datos distribuidos. Consulte ¿Qué es un almacén de lago de datos? .</td><td>2</td></tr><tr><td>Independientemente de si está generando paneles o potenciando aplicaciones de inteligencia artificial, la ingeniería de datos proporciona la red troncal para las empresas centradas en datos al asegurarse de que los datos están disponibles, limpios y almacenados en modelos de datos para una detección y uso eficaces. Azure Databricks combina la eficacia de Apache Spark con Delta y herramientas personalizadas para proporcionar una experiencia ETL inigualable. Use SQL, Python y Scala para componer la lógica ETL y organizar la implementación de trabajos programados con unos pocos clics.\n",
       "\n",
       "Las canalizaciones declarativas de Spark de Lakeflow simplifican aún más ETL mediante la administración inteligente de dependencias entre conjuntos de datos e implementan y escalan automáticamente la infraestructura de producción para garantizar una entrega de datos oportuna y precisa a las especificaciones.</td><td>3</td></tr><tr><td>Azure Databricks proporciona herramientas personalizadas para ingesta de datos , incluida Auto Loader , una herramienta eficaz y escalable para cargar datos de forma incremental e idempotente desde el almacenamiento de objetos en la nube y lagos de datos en el centro de lago de datos.\n",
       "\n",
       "Aprendizaje automático, inteligencia artificial y ciencia de datos\n",
       "\n",
       "El aprendizaje automático de Azure Databricks amplía la funcionalidad básica de la plataforma con un conjunto de herramientas adaptadas a las necesidades de los científicos de datos e ingenieros de aprendizaje automático, incluidos MLflow y Databricks Runtime para Machine Learning .\n",
       "\n",
       "Modelos de lenguaje grandes e inteligencia artificial generativa</td><td>4</td></tr><tr><td>Databricks Runtime para Machine Learning incluye bibliotecas como Hugging Face Transformers que permiten integrar modelos previamente entrenados existentes u otras bibliotecas de código abierto en el flujo de trabajo. La integración de MLflow de Databricks facilita el uso del servicio de seguimiento de MLflow con canalizaciones, modelos y componentes de procesamiento de transformadores. Integre modelos o soluciones de OpenAI de asociados como John Snow Labs en los flujos de trabajo de Databricks.\n",
       "\n",
       "Con Azure Databricks, personalice un LLM en función de sus datos para su tarea específica. Con el apoyo de herramientas de código abierto, como Hugging Face y DeepSpeed, puede tomar eficazmente un modelo básico de LLM y comenzar a entrenar con sus propios datos para mejorar la precisión en su dominio y carga de trabajo.</td><td>5</td></tr><tr><td>Además, Azure Databricks proporciona funciones de inteligencia artificial que los analistas de datos de SQL pueden usar para acceder a los modelos LLM, como desde OpenAI, directamente dentro de sus canalizaciones de datos y flujos de trabajo. Consulte Aplicación de inteligencia artificial en datos mediante Azure Databricks AI Functions .</td><td>6</td></tr><tr><td>Azure Databricks combina interfaces de usuario fáciles de usar con recursos de proceso rentables y almacenamiento asequible infinitamente escalable para proporcionar una plataforma eficaz para ejecutar consultas analíticas. Los administradores configuran clústeres de proceso escalables como almacenes de SQL , lo que permite a los usuarios finales ejecutar consultas sin preocuparse por ninguna de las complejidades de trabajar en la nube. Los usuarios de SQL pueden ejecutar consultas en los datos del almacén de lago de datos mediante el editor de consultas SQL o en cuadernos. Los cuadernos admiten Python, R y Scala, además de SQL, y permiten a los usuarios insertar las mismas visualizaciones disponibles en paneles heredados junto con vínculos, imágenes y comentarios escritos en Markdown.\n",
       "\n",
       "Gobernanza de datos y uso compartido seguro de datos</td><td>7</td></tr><tr><td>El catálogo de Unity proporciona un modelo unificado de gobernanza de datos para el almacén de lago de datos. Los administradores de la nube configuran e integran permisos de control de acceso generales para el catálogo de Unity y, a continuación, los administradores de Azure Databricks pueden administrar los permisos para los equipos e individuos. Los privilegios se administran con listas de control de acceso (ACL) a través de interfaces de usuario fáciles de usar o sintaxis SQL, lo que facilita a los administradores de bases de datos la protección del acceso a los datos sin necesidad de escalar en la administración de acceso a identidades nativas de la nube (IAM) y las redes.\n",
       "\n",
       "El catálogo de Unity facilita la ejecución de análisis seguros en la nube y proporciona una división de responsabilidades que ayuda a limitar la capacidad de reciclarse o aprender nuevas aptitudes necesarias para los administradores y los usuarios finales de la plataforma. Consulte ¿Qué es Unity Catalog?</td><td>8</td></tr><tr><td>El almacén de lago de datos hace que el uso compartido de datos en su organización sea tan sencillo como conceder acceso de consulta a una tabla o vista. Para el uso compartido fuera del entorno seguro, el catálogo de Unity incluye una versión administrada de Delta Sharing .\n",
       "\n",
       "Cada uno de los ciclos de vida de desarrollo para canalizaciones de ETL, modelos de ML y paneles de análisis presenta sus propios desafíos únicos. Azure Databricks permite a todos los usuarios aprovechar un único origen de datos, lo que reduce la duplicación de esfuerzos y los informes fuera de sincronización. Al proporcionar además un conjunto de herramientas comunes para el control de versiones, la automatización, la programación, la implementación de código y los recursos de producción, puede simplificar la sobrecarga de supervisión, orquestación y operaciones.</td><td>9</td></tr><tr><td>Los trabajos programan cuadernos de Azure Databricks, consultas SQL y otro código arbitrario. Los conjuntos de recursos de Databricks permiten definir, implementar y ejecutar recursos de Databricks, como trabajos y canalizaciones mediante programación. Las carpetas de Git le permiten sincronizar los proyectos de Azure Databricks con varios proveedores de Git populares.\n",
       "\n",
       "Para conocer los procedimientos recomendados y recomendaciones de CI/CD, consulte Procedimientos recomendados y flujos de trabajo de CI/CD recomendados en Databricks . Para obtener información general completa sobre las herramientas para desarrolladores, consulte Desarrollo en Databricks .</td><td>10</td></tr><tr><td>Azure Databricks aprovecha Structured Streaming de Apache Spark para trabajar con datos de streaming y cambios incrementales de datos. Structured Streaming se integra estrechamente con Delta Lake, y estas tecnologías proporcionan las bases para Lakeflow Spark Declarative Pipelines y Auto Loader. Consulte los conceptos de Structured Streaming .\n",
       "\n",
       "Lakebase es una base de datos de procesamiento transaccional en línea (OLTP) que está totalmente integrada con databricks Data Intelligence Platform. Esta base de datos postgres totalmente administrada permite crear y administrar bases de datos OLTP almacenadas en el almacenamiento administrado por Azure Databricks. Consulte ¿Qué es Lakebase? .\n",
       "\n",
       "¿Desea intentar usar Ask Learn para aclarar o guiarle a través de este tema?</td><td>11</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "Azure Databricks es una plataforma de análisis unificada y abierta para crear, implementar, compartir y mantener soluciones de datos, análisis e IA de nivel empresarial a escala. Databricks Data Intelligence Platform se integra con el almacenamiento en la nube y la seguridad de su cuenta en la nube, y administra e implementa la infraestructura en la nube para usted.\n\nAzure Databricks usa inteligencia artificial generativa con el almacén de lago de datos para comprender la semántica única de los datos. A continuación, optimiza automáticamente el rendimiento y administra la infraestructura para adaptarla a las necesidades de su empresa.\n\nEl procesamiento de lenguaje natural aprende el idioma de su empresa, por lo que puede buscar y detectar datos haciendo una pregunta en sus propias palabras. La asistencia del lenguaje natural le ayuda a escribir código, solucionar errores y encontrar respuestas en la documentación.",
         0
        ],
        [
         "Databricks se compromete a la comunidad de código abierto y administra las actualizaciones de integraciones de código abierto con las versiones de Databricks Runtime. Las siguientes tecnologías son proyectos de código abierto que crearon al principio empleados de Databricks:\n\nLos siguientes casos de uso resaltan algunas de las formas en que los clientes usan Azure Databricks para realizar tareas esenciales para procesar, almacenar y analizar los datos que impulsan las decisiones y las funciones empresariales críticas.\n\nCreación de un almacén de lago de datos empresarial",
         1
        ],
        [
         "Data Lakehouse combina almacenes de datos empresariales y lagos de datos para acelerar, simplificar y unificar soluciones de datos empresariales. Los ingenieros de datos, los científicos de datos, los analistas y los sistemas de producción pueden usar el data lakehouse como origen único de verdad, lo que proporciona acceso a datos coherentes y reduce las complejidades de creación, mantenimiento y sincronización de muchos sistemas de datos distribuidos. Consulte ¿Qué es un almacén de lago de datos? .",
         2
        ],
        [
         "Independientemente de si está generando paneles o potenciando aplicaciones de inteligencia artificial, la ingeniería de datos proporciona la red troncal para las empresas centradas en datos al asegurarse de que los datos están disponibles, limpios y almacenados en modelos de datos para una detección y uso eficaces. Azure Databricks combina la eficacia de Apache Spark con Delta y herramientas personalizadas para proporcionar una experiencia ETL inigualable. Use SQL, Python y Scala para componer la lógica ETL y organizar la implementación de trabajos programados con unos pocos clics.\n\nLas canalizaciones declarativas de Spark de Lakeflow simplifican aún más ETL mediante la administración inteligente de dependencias entre conjuntos de datos e implementan y escalan automáticamente la infraestructura de producción para garantizar una entrega de datos oportuna y precisa a las especificaciones.",
         3
        ],
        [
         "Azure Databricks proporciona herramientas personalizadas para ingesta de datos , incluida Auto Loader , una herramienta eficaz y escalable para cargar datos de forma incremental e idempotente desde el almacenamiento de objetos en la nube y lagos de datos en el centro de lago de datos.\n\nAprendizaje automático, inteligencia artificial y ciencia de datos\n\nEl aprendizaje automático de Azure Databricks amplía la funcionalidad básica de la plataforma con un conjunto de herramientas adaptadas a las necesidades de los científicos de datos e ingenieros de aprendizaje automático, incluidos MLflow y Databricks Runtime para Machine Learning .\n\nModelos de lenguaje grandes e inteligencia artificial generativa",
         4
        ],
        [
         "Databricks Runtime para Machine Learning incluye bibliotecas como Hugging Face Transformers que permiten integrar modelos previamente entrenados existentes u otras bibliotecas de código abierto en el flujo de trabajo. La integración de MLflow de Databricks facilita el uso del servicio de seguimiento de MLflow con canalizaciones, modelos y componentes de procesamiento de transformadores. Integre modelos o soluciones de OpenAI de asociados como John Snow Labs en los flujos de trabajo de Databricks.\n\nCon Azure Databricks, personalice un LLM en función de sus datos para su tarea específica. Con el apoyo de herramientas de código abierto, como Hugging Face y DeepSpeed, puede tomar eficazmente un modelo básico de LLM y comenzar a entrenar con sus propios datos para mejorar la precisión en su dominio y carga de trabajo.",
         5
        ],
        [
         "Además, Azure Databricks proporciona funciones de inteligencia artificial que los analistas de datos de SQL pueden usar para acceder a los modelos LLM, como desde OpenAI, directamente dentro de sus canalizaciones de datos y flujos de trabajo. Consulte Aplicación de inteligencia artificial en datos mediante Azure Databricks AI Functions .",
         6
        ],
        [
         "Azure Databricks combina interfaces de usuario fáciles de usar con recursos de proceso rentables y almacenamiento asequible infinitamente escalable para proporcionar una plataforma eficaz para ejecutar consultas analíticas. Los administradores configuran clústeres de proceso escalables como almacenes de SQL , lo que permite a los usuarios finales ejecutar consultas sin preocuparse por ninguna de las complejidades de trabajar en la nube. Los usuarios de SQL pueden ejecutar consultas en los datos del almacén de lago de datos mediante el editor de consultas SQL o en cuadernos. Los cuadernos admiten Python, R y Scala, además de SQL, y permiten a los usuarios insertar las mismas visualizaciones disponibles en paneles heredados junto con vínculos, imágenes y comentarios escritos en Markdown.\n\nGobernanza de datos y uso compartido seguro de datos",
         7
        ],
        [
         "El catálogo de Unity proporciona un modelo unificado de gobernanza de datos para el almacén de lago de datos. Los administradores de la nube configuran e integran permisos de control de acceso generales para el catálogo de Unity y, a continuación, los administradores de Azure Databricks pueden administrar los permisos para los equipos e individuos. Los privilegios se administran con listas de control de acceso (ACL) a través de interfaces de usuario fáciles de usar o sintaxis SQL, lo que facilita a los administradores de bases de datos la protección del acceso a los datos sin necesidad de escalar en la administración de acceso a identidades nativas de la nube (IAM) y las redes.\n\nEl catálogo de Unity facilita la ejecución de análisis seguros en la nube y proporciona una división de responsabilidades que ayuda a limitar la capacidad de reciclarse o aprender nuevas aptitudes necesarias para los administradores y los usuarios finales de la plataforma. Consulte ¿Qué es Unity Catalog?",
         8
        ],
        [
         "El almacén de lago de datos hace que el uso compartido de datos en su organización sea tan sencillo como conceder acceso de consulta a una tabla o vista. Para el uso compartido fuera del entorno seguro, el catálogo de Unity incluye una versión administrada de Delta Sharing .\n\nCada uno de los ciclos de vida de desarrollo para canalizaciones de ETL, modelos de ML y paneles de análisis presenta sus propios desafíos únicos. Azure Databricks permite a todos los usuarios aprovechar un único origen de datos, lo que reduce la duplicación de esfuerzos y los informes fuera de sincronización. Al proporcionar además un conjunto de herramientas comunes para el control de versiones, la automatización, la programación, la implementación de código y los recursos de producción, puede simplificar la sobrecarga de supervisión, orquestación y operaciones.",
         9
        ],
        [
         "Los trabajos programan cuadernos de Azure Databricks, consultas SQL y otro código arbitrario. Los conjuntos de recursos de Databricks permiten definir, implementar y ejecutar recursos de Databricks, como trabajos y canalizaciones mediante programación. Las carpetas de Git le permiten sincronizar los proyectos de Azure Databricks con varios proveedores de Git populares.\n\nPara conocer los procedimientos recomendados y recomendaciones de CI/CD, consulte Procedimientos recomendados y flujos de trabajo de CI/CD recomendados en Databricks . Para obtener información general completa sobre las herramientas para desarrolladores, consulte Desarrollo en Databricks .",
         10
        ],
        [
         "Azure Databricks aprovecha Structured Streaming de Apache Spark para trabajar con datos de streaming y cambios incrementales de datos. Structured Streaming se integra estrechamente con Delta Lake, y estas tecnologías proporcionan las bases para Lakeflow Spark Declarative Pipelines y Auto Loader. Consulte los conceptos de Structured Streaming .\n\nLakebase es una base de datos de procesamiento transaccional en línea (OLTP) que está totalmente integrada con databricks Data Intelligence Platform. Esta base de datos postgres totalmente administrada permite crear y administrar bases de datos OLTP almacenadas en el almacenamiento administrado por Azure Databricks. Consulte ¿Qué es Lakebase? .\n\n¿Desea intentar usar Ask Learn para aclarar o guiarle a través de este tema?",
         11
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "chunk_text",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "chunk_id",
         "type": "\"long\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Creamos un DataFrame Spark a partir de la lista de chunks\n",
    "# Cada chunk será una fila en la columna \"chunk_text\"\n",
    "df_chunks = spark.createDataFrame([(c,) for c in chunks], [\"chunk_text\"])\n",
    "\n",
    "# Agregamos una columna \"chunk_id\" con un identificador único para cada fila\n",
    "# `monotonically_increasing_id()` genera un ID único creciente para cada chunk\n",
    "df_chunks = df_chunks.withColumn(\"chunk_id\", monotonically_increasing_id())\n",
    "\n",
    "# Confirmamos que el DataFrame se creó correctamente mostrando las primeras filas\n",
    "print(\"✅ DataFrame Spark de chunks creado:\")\n",
    "display(df_chunks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "10a31d80-bc42-4a65-aae8-f8d9079d29ca",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 5. Guardar tabla Delta en Capa Silver\n",
    "Guardamos el resultado procesado en `dev.silver.rag_chunks` en formato **Delta**.\n",
    "Esto permite:\n",
    "1.  **Reutilización:** Si falla el proceso de generación de embeddings, no hay que volver a leer ni procesar el texto.\n",
    "2.  **Schema Enforcement:** Aseguramos que siempre tengamos las columnas `chunk_text` y `chunk_id`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f2e56a83-a3ac-4c4c-b4e1-f2b2cb277c99",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\uD83D\uDCBE Guardando tabla Delta en: dev.silver.rag_chunks ...\n✅ Proceso finalizado exitosamente.\n"
     ]
    }
   ],
   "source": [
    "target_table = \"dev.silver.rag_chunks\"\n",
    "\n",
    "print(f\"\uD83D\uDCBE Guardando tabla Delta en: {target_table} ...\")\n",
    "\n",
    "df_chunks.write \\\n",
    "    .format(\"delta\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .option(\"overwriteSchema\", \"true\") \\\n",
    "    .saveAsTable(target_table)\n",
    "\n",
    "print(\"✅ Proceso finalizado exitosamente.\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "10_rag_ingesta_y_chunking",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}