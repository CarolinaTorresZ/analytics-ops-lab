# Databricks notebook source
# MAGIC %md
# MAGIC # Parte 2:  Sistema RAG - Recuperaci√≥n y Generaci√≥n (Inferencia)
# MAGIC *   **Autor:** Carolina Torres Zapata
# MAGIC *   **Fecha:** 2025-11-24
# MAGIC *   **Contexto:** Este es el componente final del sistema ("The App"). Aqu√≠ integramos la **Base de Conocimiento** construida previamente con un **LLM Generativo** (Llama 3 o similar) para responder preguntas de usuario.
# MAGIC *   **Objetivos del Notebook:**
# MAGIC      1.  **Recuperaci√≥n (Retrieval):** Implementar un motor de b√∫squeda vectorial en memoria (r√°pido y eficiente) usando similitud de coseno.
# MAGIC      2.  **Generaci√≥n (Generation):** Conectar con un LLM mediante `Databricks Serving Endpoints`.
# MAGIC      3.  **Grounding (Seguridad):** Dise√±ar un *System Prompt* estricto que obligue al modelo a responder **solo** con la informaci√≥n suministrada, mitigando alucinaciones.
# MAGIC
# MAGIC

# COMMAND ----------

# MAGIC %md
# MAGIC ## 0. Configuraci√≥n del Entorno y Dependencias
# MAGIC
# MAGIC Antes de iniciar el flujo de inferencia, aseguramos que el cluster tenga las herramientas necesarias para la orquestaci√≥n del RAG.
# MAGIC
# MAGIC *   **`sentence-transformers`**: Motor local para vectorizar la pregunta del usuario (debe coincidir con la versi√≥n usada en la ingesta).
# MAGIC *   **`databricks-sdk[openai]`**: Cliente oficial para interactuar con los **Serving Endpoints** (LLMs) de Databricks de forma segura.
# MAGIC
# MAGIC **Nota Operativa:** Se ejecuta `dbutils.library.restartPython()` para reiniciar el proceso de Python y forzar la carga de las nuevas librer√≠as instaladas sin necesidad de reiniciar todo el cluster.

# COMMAND ----------

# INSTALACI√ìN DE LIBRER√çAS
# sentence-transformers: Para vectorizar la pregunta.
# databricks-sdk[openai]: Cliente necesario para hablar con Llama 3.
# mlflow: Para registro de experimentos.
%pip install -U -q sentence-transformers "databricks-sdk[openai]" mlflow databricks-agents

# REINICIO DEL KERNEL
# Obligatorio para aplicar cambios. Al terminar esta celda, la memoria se limpia.
dbutils.library.restartPython()

# COMMAND ----------

# MAGIC %md
# MAGIC ## 1. Importar Librer√≠as

# COMMAND ----------

import numpy as np
import pandas as pd
import mlflow
from sentence_transformers import SentenceTransformer
from databricks.sdk import WorkspaceClient

# COMMAND ----------

# MAGIC %md
# MAGIC ## 2. Conexi√≥n con el LLM
# MAGIC En un entorno operativo, los endpoints pueden cambiar de nombre o estar inactivos.
# MAGIC Implementamos una l√≥gica de **"Health Check"**: iteramos sobre una lista de modelos aprobados (priorizando Llama 3-70B) y nos conectamos al primero que responda exitosamente. Esto evita que el pipeline falle por un error de configuraci√≥n est√°tica.

# COMMAND ----------

# CONFIGURACI√ìN DEL MODELO LLM
LLM_ENDPOINT_NAME = None

def is_endpoint_available(endpoint_name):
    """Verifica si un endpoint responde."""
    try:
        client = WorkspaceClient().serving_endpoints.get_open_ai_client()
        client.chat.completions.create(
            model=endpoint_name, 
            messages=[{"role": "user", "content": "Test"}]
        )
        return True
    except Exception:
        return False

print("üîÑ Buscando endpoint activo...")

# Lista de candidatos (Llama 3 es la prioridad)
candidates = [
    "databricks-meta-llama-3-3-70b-instruct", 
    "databricks-meta-llama-3-1-70b-instruct",
    "databricks-claude-3-7-sonnet"
]

for candidate in candidates:
    if is_endpoint_available(candidate):
        LLM_ENDPOINT_NAME = candidate
        break

# Validaci√≥n estricta: Si no hay modelo, detenemos el notebook
assert LLM_ENDPOINT_NAME is not None, "‚ùå No se encontr√≥ ning√∫n modelo activo."

print(f"üöÄ Conectado exitosamente a: {LLM_ENDPOINT_NAME}")

# COMMAND ----------

# MAGIC %md
# MAGIC ## 3. Carga de Recursos (Base de Conocimiento)
# MAGIC Inicializaci√≥n del Motor de B√∫squeda (In-Memory)
# MAGIC Cargamos los dos componentes cr√≠ticos para la fase de recuperaci√≥n:
# MAGIC 1.  **Base de Conocimiento (Vectores):** La tabla Silver `rag_embeddings` convertida a matrices de NumPy para c√°lculos matem√°ticos ultrarr√°pidos.
# MAGIC 2.  **Encoder (Modelo de Embeddings):** El mismo modelo `all-MiniLM-L6-v2` usado en la ingesta. *Nota: Es vital usar exactamente el mismo modelo para que los vectores sean comparables.*

# COMMAND ----------

#  CARGA DE MOTOR DE B√öSQUEDA

print("‚è≥ Cargando recursos en memoria...")

# A. Cargar Tabla Silver (Knowledge Base)
TABLA_VECTORES = "dev.silver.rag_embeddings" 

try:
    df_kb = spark.read.table(TABLA_VECTORES).toPandas()
    
    # Convertir a matriz NumPy para velocidad
    kb_matrix = np.stack(df_kb["embedding"].values)
    kb_texts = df_kb["chunk_text"].values
    
    print(f"   ‚úÖ Base de datos cargada: {len(kb_texts)} documentos.")

    # B. Cargar Modelo de Embeddings (Local)
    embedding_model = SentenceTransformer('all-MiniLM-L6-v2')
    
    print("   ‚úÖ Modelo de vectorizaci√≥n (all-MiniLM-L6-v2) listo.")

except Exception as e:
    print(f"‚ùå Error cargando recursos: {e}")

# COMMAND ----------

# MAGIC %md
# MAGIC ##  4. L√≥gica del Sistema RAG (Funciones)
# MAGIC Implementamos la b√∫squeda sem√°ntica mediante **Producto Punto (Similitud de Coseno)**.
# MAGIC El flujo es:
# MAGIC 1.  El usuario hace una pregunta -> Se convierte en vector.
# MAGIC 2.  Comparamos ese vector contra los 12 vectores de nuestra base de datos.
# MAGIC 3.  Seleccionamos los `k=3` fragmentos m√°s similares (con mayor puntaje).
# MAGIC
# MAGIC Aplicamos t√©cnicas de **Prompt Engineering** para soporte operativo:
# MAGIC *   **Rol:** "Asistente T√©cnico experto en Databricks".
# MAGIC *   **Restricci√≥n Negativa:** Si el contexto no tiene la respuesta, el modelo debe admitirlo expl√≠citamente (*"La informaci√≥n disponible no menciona..."*). Esto es crucial para evitar enga√±ar al usuario.

# COMMAND ----------

# DEFINICI√ìN DE FUNCIONES (CORE)

def recuperar_contexto(pregunta, k=3):
    """Vectoriza la pregunta y busca los 3 fragmentos m√°s similares."""
    # 1. Vectorizar
    query_vector = embedding_model.encode(pregunta)
    
    # 2. Similitud (Producto Punto)
    scores = np.dot(kb_matrix, query_vector)
    
    # 3. Ranking
    top_indices = np.argsort(scores)[-k:][::-1]
    
    return [kb_texts[i] for i in top_indices]


def sistema_rag(pregunta):
    """Orquestador: Pregunta -> Contexto -> LLM -> Respuesta"""
    print(f"üîé Analizando: '{pregunta}'")
    
    # PASO 1: RETRIEVAL
    chunks = recuperar_contexto(pregunta, k=3)
    contexto_str = "\n\n".join(chunks)
    
    print(f"   üìÑ Contexto encontrado: {len(chunks)} fragmentos.")
    
    # PASO 2: GENERATION
    try:
        w = WorkspaceClient()
        client = w.serving_endpoints.get_open_ai_client()
        
        # Prompt del Sistema (Reglas para el LLM)
        system_instructions = f"""
        Eres un Asistente T√©cnico experto en Databricks.
        Responde a la pregunta del usuario bas√°ndote √öNICAMENTE en el contexto proporcionado abajo.
        
        Reglas:
        1. Si la respuesta est√° en el contexto, expl√≠cala claramente en espa√±ol.
        2. Si la respuesta NO est√° en el contexto, di textualmente: "La informaci√≥n disponible no menciona este tema".
        3. No inventes informaci√≥n.
        
        CONTEXTO:
        {contexto_str}
        """
        
        response = client.chat.completions.create(
            model=LLM_ENDPOINT_NAME,
            messages=[
                {"role": "system", "content": system_instructions},
                {"role": "user", "content": pregunta}
            ],
            temperature=0.1, 
            max_tokens=500
        )
        
        respuesta_final = response.choices[0].message.content
        
        # Salida visual
        print("\n" + "="*60)
        print("ü§ñ RESPUESTA GENERADA:")
        print("="*60)
        print(respuesta_final)
        print("-" * 60)
        
    except Exception as e:
        print(f"‚ùå Error en la generaci√≥n: {e}")

# COMMAND ----------

# MAGIC %md
# MAGIC ##5. Pruebas (Interacci√≥n)
# MAGIC Ejecutamos escenarios de prueba para validar el comportamiento del sistema:
# MAGIC 1.  **Caso Positivo:** Pregunta sobre "Unity Catalog" (informaci√≥n presente en el documento). Se espera una respuesta t√©cnica y precisa.
# MAGIC 2.  **Caso Negativo (Control):** Pregunta sobre "DBUs" (concepto de facturaci√≥n no presente en el texto introductorio). Se espera que el sistema active la cl√°usula de seguridad y **no** invente una respuesta.

# COMMAND ----------

# Pregunta 1: Sobre Gobernanza
sistema_rag("¬øPara qu√© sirve Unity Catalog?")

# Pregunta 2: Sobre Infraestructura
sistema_rag("¬øQu√© son las DBUs?")

sistema_rag("¬øQu√© servicios ofrece Azure Databricks?")